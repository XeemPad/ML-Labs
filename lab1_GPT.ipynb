{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8dbe87",
   "metadata": {
    "id": "3e8dbe87"
   },
   "source": [
    "## Обучение генеративной трансформерной модели с помощью `transformers`\n",
    "\n",
    "В этой работе мы познакомимся на практике с процессом тренировки большой трансформерной языковой модели. Поскольку такая тренировка требует существенных вычислительных ресурсов, выполнять эту работу рекомендуется в Yandex DataSphere, в которой доступны вычислитльные узлы с одни или двумя графическими процессорами Tesla V100.\n",
    "\n",
    "### Архитектура трансформеров\n",
    "\n",
    "В рамках этой работы мы предполагаем, что вы уже знакомы с архитектурой трансформеров, например, по [статье из ML-хэндбука](https://academy.yandex.ru/handbook/ml/article/transformery). Также для первоначального знакомства рекомендую заметку [Jay Alammar. The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/), и её частичный [русскоязычный перевод](https://habr.com/ru/articles/486358/).\n",
    "\n",
    "Мы не будем в рамках работы создавать архитетуру нейросети \"с нуля\". Если вам инетересно изучить реализацию трансформеров - рекомендую посмотреть на [NanoGPT](https://github.com/karpathy/nanoGPT). Подробно эта реализация разбирается в [этом видео](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
    "\n",
    "### Библиотека `transformers` и её друзья\n",
    "\n",
    "Стандартом де факто в реализации трансформеров служит библиотека `transformers` от [HuggingFace](http://huggingface.co). Она содержит в себе реализацию большого количества используемых трансформерных архитектур, а также ряд полезных инструментов для их обучения. Многие инструменты также оформлены в виде отдельных библиотек, которые хорошо работают вместе:\n",
    "\n",
    "* `tokenizers` - быстрая реализация различных токенизаторов, позволяющих разделять входной текст на токены\n",
    "* `datasets` - манипулирование большими датасетами\n",
    "* `evaluate` - вычисление различных метрик и оценка результатов обучения\n",
    "* `accelerate` - реализация вычислений на множестве GPU и на вычислительных кластерах\n",
    "\n",
    "Для начала, установим необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5909a2",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18835,
     "status": "ok",
     "timestamp": 1737286484468,
     "user": {
      "displayName": "Xeem Pad",
      "userId": "14219459591791707475"
     },
     "user_tz": -180
    },
    "id": "ac5909a2",
    "outputId": "7b592db6-e3fd-4e65-f705-90fd80db21ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2024.12.0 requires fsspec==2024.12.0.*, but you have fsspec 2024.9.0 which is incompatible.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.9.0\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers tokenizers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c343afb",
   "metadata": {
    "id": "4c343afb"
   },
   "source": [
    "В текущем варианте при работе в DataSphere возникают проблемы при использовании файлового хранилища. Для решения проблем нам нужно установить последнюю версию библиотеки `s3fs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a581a6a",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a581a6a",
    "outputId": "c4120e17-3aeb-4f7a-c824-9a2155236e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/dask/s3fs\n",
      "  Cloning https://github.com/dask/s3fs to /tmp/pip-req-build-4r4xrjop\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/dask/s3fs /tmp/pip-req-build-4r4xrjop\n",
      "  Resolved https://github.com/dask/s3fs to commit 51e3c80ef380a82081a171de652e2b699753be2b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.11/dist-packages (from s3fs==2024.12.0) (2.18.0)\n",
      "Collecting fsspec==2024.12.0.* (from s3fs==2024.12.0)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from s3fs==2024.12.0) (3.11.11)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.12.0) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.36.2,>=1.36.0 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.12.0) (1.36.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.12.0) (2.8.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.12.0) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.12.0) (6.1.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.12.0) (2.3.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.12.0) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.12.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.12.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.12.0) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.12.0) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.12.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.12.0) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs==2024.12.0) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.12.0) (3.10)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Installing collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.9.0\n",
      "    Uninstalling fsspec-2024.9.0:\n",
      "      Successfully uninstalled fsspec-2024.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.2.0 requires fsspec[http]<=2024.9.0,>=2023.1.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.12.0\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade git+https://github.com/dask/s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f15e8",
   "metadata": {
    "id": "bc3f15e8"
   },
   "source": [
    "### Подготовка датасета\n",
    "\n",
    "В нашем примере, мы будем обучать виртуального Льва Толстого. Для этого, возьмём все основные романы писателя, и подготовим их них датасет. В качестве отправной точки будет использовать тексты из [библиотеки Мошкова](http://lib.ru). Соберем ссылки на романы Анна Каренина, Война и Мир и др. в один список:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7ffcc",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "44d7ffcc"
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0039.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0040.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0050.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0060.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0070.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0080.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0090.shtml\",\n",
    "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_1860_dekabristy.shtml\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305027b2",
   "metadata": {
    "id": "305027b2"
   },
   "source": [
    "Теперь скачаем все материалы и подготовим из них один большой текстовый файл. Для того нам понадобится убрать HTML-теги, а также несколько первоначальных строчек в каждом из файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed7313",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "84ed7313"
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    return requests.get(url).text\n",
    "\n",
    "\n",
    "# code borrowed from here: https://github.com/pallets/markupsafe/blob/0.23/markupsafe/__init__.py#L21\n",
    "striptags_re = re.compile(r\"(<!--.*?-->|<[^>]*>)\")\n",
    "entity_re = re.compile(r\"&([^;]+);\")\n",
    "\n",
    "\n",
    "def to_text(s):\n",
    "    return html.unescape(striptags_re.sub(\"\", s))\n",
    "\n",
    "\n",
    "def beautify(s):\n",
    "    lines = [x.strip() for x in s.split(\"\\n\") if x.strip() != \"\"]\n",
    "    for i in range(min(100, len(lines))):\n",
    "        if lines[i] == \"-->\":\n",
    "            break\n",
    "    return \"\\n\".join(lines[i + 1 :] if i < 100 else lines)\n",
    "\n",
    "\n",
    "with open(\"dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for u in urls:\n",
    "        text = beautify(to_text(download(u)))\n",
    "        f.write(text + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FuA2quf0PPgC",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FuA2quf0PPgC",
    "outputId": "5d08e114-d968-4e30-9302-7c40661a140f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лев Николаевич Толстой\n",
      "Семейное счастье\n",
      "Оригинал текста: в электронной библиотеке Олега Колесникова\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset.txt\", encoding=\"utf-8\") as f:\n",
    "      text = f.read()\n",
    "      print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd8e6a",
   "metadata": {
    "id": "32dd8e6a"
   },
   "source": [
    "В результате мы получили один большой файл `dataset.txt`, содержащий большой корпус текстов Льва Толстого.\n",
    "\n",
    "### Датасеты в Yandex DataSphere\n",
    "\n",
    "При использовании Yandex DataSphere, у нас ограничен объем данных, которые мы можем хранить вместе с проектом. Обычно, большие объемы данных в облаке хранят в **объектном хранилище S3**. DataSphere позволяет легко подключаться к таким хранилищам, монтируя их как обычную директорию в проекте, после чего можно получить доступ к данным как к обычным файлам.\n",
    "\n",
    "Однако, доступ в хранилище S3 не слишком быстрый, а для обучения сетей хочется отдавать данные как можно быстрее, не тормозя вычислительный процесс. Для этого в DataSphere предусмотрены **датасеты** - это отдельные виртуальные накопители, которые можно легко подключать к различным вычислительным ресурсам.\n",
    "\n",
    "Будучи созданным, датасет не может быть изменён - это обеспечивает сохранность исходных данных. Хорошим стилем считается хранить все данные для обучения моделей в датасетах. Кроме того, датасеты можно разделять между другими участниками сообщества или проекта.\n",
    "\n",
    "В нашем случае объем обучающих данных небольшой, и можно обойтись без создания датасета. Но если вы хотите попробовать - добавьте ниже ячейку со следующим кодом и запустите его:\n",
    "```\n",
    "#!:bash\n",
    "#pragma dataset init mytext --size 1Gb\n",
    "cp dataset.txt /home/jupyter/mnt/datasets/mytext\n",
    "```\n",
    "Это создаст датасет `mytext` с единственным файлом `dataset.txt`. При этом ниже в коде вам нужно будет изменить путь к файлу `dataset.txt` на `/home/jupyter/mnt/datasets/mytext/dataset.txt`.\n",
    "\n",
    "> Кажется, что в создании датасета нет большого смысла, поскольку мы просто положили тот же файл в другое место. На самом деле это не так - теперь файл `dataset.txt` не будет занимать место в хранилище проекта, доступ к нему будет быстрее, а также вы сможете легко поделиться датасетом с другими участниками команды, чтобы им не пришлось писать код по предварительной обработке данных. При этом датасет не будет копироваться, а будет просто смонтирован в соответствующие директории в DataSphere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b835e2",
   "metadata": {
    "id": "05b835e2"
   },
   "source": [
    "### Токенизация\n",
    "\n",
    "Нейросети работают с числами, поэтому первым этапом является токенизация текста, т.е. разбиение его на атомарные элементы, которые затем можно добавить в словарь, и представлять текст как последовательность индексов в словаре. Текст можно токенизировать по буквам, или по словам.\n",
    "\n",
    "При построении современных генеративных сетей текст обычно разбивают на фрагменты таким образом, чтобы частота появления каждого фрагмента в тексте была примерно одинакова. Это лежит в основе т.н. Byte-Pair Encoding (BPE). Подробнее можно прочитать [в этой статье](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt).\n",
    "\n",
    "Для обучения своего токенизатора используем библиотеку `tokenizers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d25b461",
   "metadata": {
    "executionInfo": {
     "elapsed": 10739,
     "status": "ok",
     "timestamp": 1739655036428,
     "user": {
      "displayName": "Xeem Pad",
      "userId": "14219459591791707475"
     },
     "user_tz": -180
    },
    "id": "7d25b461"
   },
   "outputs": [],
   "source": [
    "import tokenizers as tok\n",
    "import transformers as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470b754",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "e470b754"
   },
   "outputs": [],
   "source": [
    "tokenizer = tok.Tokenizer(tok.models.BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = tok.pre_tokenizers.Whitespace()\n",
    "trainer = tok.trainers.BpeTrainer(special_tokens=[\"[PAD]\"])\n",
    "tokenizer.train([\"dataset.txt\"], trainer)\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147ea7e",
   "metadata": {
    "id": "9147ea7e"
   },
   "source": [
    "А данном случае мы используем два специальных токена - `[UNK]` для представления неизвестного токена (такое случится, если на вход попадёт символ, который токенизатор не видел при обучении), и `[PAD]` для **паддинга** - он используется, если нужно дополнить последовательность до определённой длины.\n",
    "\n",
    "Вот как можно закодировать входной текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04579048",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "04579048",
    "outputId": "c88a5e59-7e09-4e93-cc9c-9f4b5454cb6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Иван',\n",
       " 'С',\n",
       " 'иг',\n",
       " 'изму',\n",
       " 'н',\n",
       " 'до',\n",
       " 'вич',\n",
       " 'подошел',\n",
       " 'к',\n",
       " 'окну',\n",
       " 'и',\n",
       " 'закашлялся',\n",
       " '.',\n",
       " 'Вечер',\n",
       " 'ело',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Иван Сигизмундович подошел к окну и закашлялся. Вечерело.\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f66e2",
   "metadata": {
    "id": "156f66e2"
   },
   "source": [
    "Видим, что популярные слова токенизируются целиком, а те, которые встречаются в тексте редко или не встречаются вовсе - разбиваются на фрагменты.\n",
    "\n",
    "### Генеративные трансформеры\n",
    "\n",
    "Для генерации текста используются архитектуры GPT - Generative Pre-trained Transformers. В то время как полноценные трансформеры являются энкодер-декодерной архитектурой, т.е. могут решать задачи преобразования одного вида последовательности в другую, GPT является только декодером, т.к. способно прогнозировать распределение вероятности следующего слова по начальной части последовательности.\n",
    "\n",
    "Мы используем архитектуру GPT-2, которая, с одной стороны, не слишком огромна, а с другой - может неплохо обучиться. Сперва попробуем натренировать такую архитетуру \"с нуля\".\n",
    "\n",
    "Дла начала нам потребуется преобразовать наш токенизатор к объекту `ttokenizer`, который понимает библиотека transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928e282",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8928e282",
    "outputId": "0617741c-52d8-42c6-be86-0339d55bd97a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "ttokenizer = tr.PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91488224-b24c-44d6-a805-b4cea7ad833a",
   "metadata": {
    "id": "91488224-b24c-44d6-a805-b4cea7ad833a"
   },
   "source": [
    "Теперь создадим непосредственно нейросетевую модель GPT2. При этом основные параметры (количество слоёв, количество голов внимания и т.д. оставим по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b18d8be",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9b18d8be"
   },
   "outputs": [],
   "source": [
    "config = tr.GPT2Config(\n",
    "    vocab_size=len(vocab),\n",
    "    bos_token_id=tokenizer.token_to_id(\"[CLS]\"),\n",
    "    eos_token_id=tokenizer.token_to_id(\"[EOS]\"),\n",
    ")\n",
    "gpt = tr.GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379893b-f03d-48cc-b09f-7c085b15f0ce",
   "metadata": {
    "id": "8379893b-f03d-48cc-b09f-7c085b15f0ce"
   },
   "source": [
    "Веса вновь созданной модели инициализируются случайным образом, поэтому если мы попросим такую модель сгенерировать текст - получится бессмыслица:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec498e",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9eec498e",
    "outputId": "51b6b3c3-ea3a-42a4-f72a-dd8f99f94e76"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Мне нравится куртке куртке гениев мальчику усмотре Ката Ката лошаден лошаден измениться жестве убеждены честолюби оголи оголи комитет добрее добрее добрее добрее добрее добрее добрее кобе кобе женный женный женный боитесь спорили grand grand раздви вынесла вынесла раздви раздви доске доске suis поднимался поднимался Вто гусару ухаживать круглыми рик рик брил Comme'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = gpt.generate(\n",
    "    **ttokenizer(\"Мне нравится \", return_tensors=\"pt\"),\n",
    "    max_new_tokens=50,\n",
    "    top_k=3,\n",
    "    do_sample=True\n",
    ")\n",
    "ttokenizer.decode(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bd5a4-3378-4a38-ac3d-07f48ad00047",
   "metadata": {
    "id": "3f4bd5a4-3378-4a38-ac3d-07f48ad00047"
   },
   "source": [
    "Теперь нам надо научиться подавать на вход модели фрагменты текста для обучения. Для этого существует библиотека `datasets`, входящее в семейство трансформерных библиотек HuggingFace. Помимо того, что эта библиотека умеет работать с разными форматами входных датасетов, она также интегрирована с HuggingFace Hub, и может в одну строчку загружать множество имеющихся на этом сайте датасетов.\n",
    "\n",
    "В нашем случае мы загрузим датасет из текстового файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f83e7a",
   "metadata": {
    "colab": {
     "background_save": true,
     "referenced_widgets": [
      "4fb5a7485c6244239e49fdb681375088"
     ]
    },
    "id": "04f83e7a",
    "outputId": "20e78b76-b06c-4461-efa7-047df4702a1b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb5a7485c6244239e49fdb681375088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Он взял своею большою рукой меня за руку, и пожал так крепко, честно, только что не больно. Я думала, что он поцелует мою руку, и нагнулась было к нему, но он еще раз пожал мне руку и прямо в глаза посмотрел своим твердым и веселым взглядом.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"text\", data_files=\"dataset.txt\")\n",
    "dataset[\"train\"][13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c042844-9998-4a05-bc69-165b77896fcf",
   "metadata": {
    "id": "1c042844-9998-4a05-bc69-165b77896fcf"
   },
   "source": [
    "Далее нам необходимо научиться токенизировать датасет, т.е. преобразовывать в числовые тензоры, которые затем мы будем подавать на вход нейросети в процессе обучения. Для этого опишем фукнцию `tokenize`, которая будет возвращать словарь с несколькими полями:\n",
    "\n",
    "* `input_ids` - это собственно номера слов входной последовательности в словаре\n",
    "* `token_type_ids` - содержит нули. Это поле используется в более сложных сценариях, например, когда мы тренируем сеть отвечать на вопросы по тексту. В этом случае нам нужно подать на вход текст + вопрос, и это поле позволяет различать между несколькими разными по смыслу фрагментами входной последовательности\n",
    "* `atttention_mask` показывает, какая часть входной последовательности значима. Для организации последовательности в minibatch нам может потребоваться дополнить последовательность до максимальной длины, и поле `attention_mask` содержит 1 в тех позициях, которые соответствуют исходной последовательности\n",
    "\n",
    "Такой формат входных данных типичен для трансформерной архитектуры. Также мы передаем последовательность значений целевой переменной `labels`, но поскольку наша задача - это генерация текста, то в качестве `labels` мы передаём копию исходного текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f7b2ad",
   "metadata": {
    "colab": {
     "background_save": true,
     "referenced_widgets": [
      "2f672071eccf4d79b6c74552685d3878"
     ]
    },
    "id": "72f7b2ad",
    "outputId": "43fdafff-a212-4012-bfe3-664f4f38fd61"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f672071eccf4d79b6c74552685d3878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [9585, 9735, 3192],\n",
       " 'token_type_ids': [0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1],\n",
       " 'labels': [9585, 9735, 3192]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    x = ttokenizer(x[\"text\"])\n",
    "    x[\"labels\"] = x[\"input_ids\"].copy()\n",
    "    return x\n",
    "\n",
    "\n",
    "ds = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800b193-896c-4b68-97cf-584cd69e1880",
   "metadata": {
    "id": "5800b193-896c-4b68-97cf-584cd69e1880"
   },
   "source": [
    "Для обучения лучше всего использовать длинные фрагменты текста, поэтому мы сгруппируем все последовательности токенов в блоки размером `block_size`. Для этого мы сначала сконкатенируем все последовательности, а потом разобъем их на блоки. В данном случае мы не будем даже разбивать последовательность на слова и/или предложения - как показывает практика, такой упрощенный подход также даёт хорошие результаты.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b55b96",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c7c3955e8d734693b3f40b6dcce0a7f9"
     ]
    },
    "executionInfo": {
     "elapsed": 4383,
     "status": "ok",
     "timestamp": 1737286021573,
     "user": {
      "displayName": "Xeem Pad",
      "userId": "14219459591791707475"
     },
     "user_tz": -180
    },
    "id": "c6b55b96",
    "outputId": "bc67a833-8eb9-47bf-8189-645ae3eb8531"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c3955e8d734693b3f40b6dcce0a7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "block_size = 512 # 1024\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "dsb = ds.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56ffd1-995d-44e0-89d6-7740a0f075b6",
   "metadata": {
    "id": "dd56ffd1-995d-44e0-89d6-7740a0f075b6"
   },
   "source": [
    "Теперь мы готовы к обучению! Для задания параметров обучения мы создаём объект `TrainingArguments`, в котором задаем директорию, куда будут записываться промежуточные результаты обучения, число эпох, скорость обучения и т.д. Затем на основе этих параметров создаём объект `Trainer`.\n",
    "\n",
    "Обратите внимание, что размер записываемой на диск сети GPT-2 может быть весьма большим (около 1.4 Gb), что может привести к исчерпанию размера вашей домашней директории в DataSphere. Исходя из этого лучше выбирать параметры `save_steps` и `num_train_epochs` таким образом, чтобы количество записываемых на диск чекпоинтов не превышало 3-5 шт.\n",
    "\n",
    "Для начала стоит попробовать пообучать сеть в течение 30-90 минут, чтобы увидеть, что она начинает складывать слова более менее правдоподобно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408488f",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3769,
     "status": "ok",
     "timestamp": 1737285600938,
     "user": {
      "displayName": "Xeem Pad",
      "userId": "14219459591791707475"
     },
     "user_tz": -180
    },
    "id": "3408488f",
    "outputId": "02d1637d-aaef-4aa3-8564-787d5610a256"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-34047ea34211>:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = tr.Trainer(\n"
     ]
    }
   ],
   "source": [
    "targs = tr.TrainingArguments(\n",
    "    output_dir=\"gpt2-scratch\",\n",
    "    num_train_epochs=30,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=200,\n",
    "    save_steps=1500,\n",
    ")\n",
    "trainer = tr.Trainer(\n",
    "    gpt,\n",
    "    args=targs,\n",
    "    train_dataset=dsb[\"train\"],\n",
    "    tokenizer=ttokenizer,\n",
    "    data_collator=tr.default_data_collator,  # tr.DataCollatorForLanguageModeling(tokenizer=ttokenizer,mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee9c3a",
   "metadata": {
    "id": "8cee9c3a"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcda58f-f57d-42b6-88bd-8316b31816be",
   "metadata": {
    "id": "4bcda58f-f57d-42b6-88bd-8316b31816be"
   },
   "source": [
    "Теперь посмотрим, как работает генерация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fba89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "executionInfo": {
     "elapsed": 3368,
     "status": "ok",
     "timestamp": 1737285612257,
     "user": {
      "displayName": "Xeem Pad",
      "userId": "14219459591791707475"
     },
     "user_tz": -180
    },
    "id": "b03fba89",
    "outputId": "286be358-479e-42d4-bf42-9bc9d46a67f2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Пьер закашлялся и заверну обнимали обнимали приобрел ори подбородок Rapp молиться ваемый тонким скал нится главы простым чрезвычай lais ален прон известное папенька почт ходь смягчи почт стенки верст принца вольте прал эскадрон тости Шере скал роте обмунди неловкость тости we вытянув разоря ходь курган дования саживаясь министерства плодо деньгами Аграф блестел Собра склад фантазии швейца малень французское свадь ответственности щеголяя вре арфе другой старичку обвяза mante эман входи чное бую прон рассматриваем генералов двугри шительно ален пальцев подо диа Челове добросовестно тела нахожусь перекрестилась АЯ считаться эскадрон сердись поезд казенный жалоб чину Адъютан самые голодом жалоб подо нится эскадрон подбородок обвяза швейца швейца рассматриваем предназначено отступление озлобле высунулась таранта развлека внутренней Бона vou адвокаты отдачи нясь смех слушавший слушавший Аркадьича Степанович жденные шительно разоря ию подбородок входи вших приездом челю высунулась чка веселиться эскадрон приближении стоит чертами одинокий victoire тонким мучали Причина реше запове мешок ходь личным повела улицами жишь Она намека'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ttokenizer = tr.PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "res = gpt.generate(\n",
    "    **ttokenizer(\"Пьер закашлялся и\", return_tensors=\"pt\").to(\"cuda\"),\n",
    "    max_new_tokens=150,\n",
    "    do_sample=True\n",
    ")\n",
    "ttokenizer.decode(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d8ea2-4d80-4c55-b2ae-ba94873d3483",
   "metadata": {
    "id": "674d8ea2-4d80-4c55-b2ae-ba94873d3483"
   },
   "source": [
    "Кажется, что сгенерированный текст пока ещё не слишком осмысленный. Но сравните его с первоначальным текстом, сгенерированным необученной нейросетью - в нём почти не было корректных грамматических конструкций. За примерно час обучения сеть уже стала неплохо понимать, какие слова хорошо сочетаются друг с другом, и в целом начала говорить более осмысленно. Помните, что трансформерная модель - сложная, и для обучения полноценной GPT-2 \"с нуля\" требуются сотни и тысячи GPU-часов.\n",
    "\n",
    "> Прежде, чем переходить к следующим экспериментам, очистим память. Если вдруг на следующем этапе возникнет переполнение памяти GPU, может потребоваться перезапуск ядра ноутбука - выберите к меню Kernel -> Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5903b42d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1072,
     "status": "ok",
     "timestamp": 1737286054068,
     "user": {
      "displayName": "Xeem Pad",
      "userId": "14219459591791707475"
     },
     "user_tz": -180
    },
    "id": "5903b42d",
    "outputId": "e7f104bd-6754-4315-bb80-461e338b2fb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gpt = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70215e43-c58e-4182-bda1-726a0b5c8939",
   "metadata": {
    "id": "70215e43-c58e-4182-bda1-726a0b5c8939"
   },
   "source": [
    "## До-обучение GPT-2\n",
    "\n",
    "За приемлемое время сложно достичь приемлемого качества обучения трансформера, поэтому обычно используют предобученные модели (поэтому в названии GPT и фигурирует слово *Pretrained*), которые уже научились \"читать\" на нужном языке, и их необходимо лишь немного \"доучить\" под требуемую предметную область или стиль. В этом случае процесс обучения модели почти не отличается от того, что мы делали ранее - с той лишь разницей, что необходимо использовать токенизатор, который использовался при обучении исходной модели.\n",
    "\n",
    "Для начала, загрузим предобученную модель **ruGPT** и соответствующий токенизатор, и посмотрим, как эта модель умеет продолжать текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd8bbd-77e9-442a-a36a-3d1883568f83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "executionInfo": {
     "elapsed": 7586,
     "status": "ok",
     "timestamp": 1737286102310,
     "user": {
      "displayName": "Xeem Pad",
      "userId": "14219459591791707475"
     },
     "user_tz": -180
    },
    "id": "bafd8bbd-77e9-442a-a36a-3d1883568f83",
    "outputId": "08228909-b5ca-4b8e-d2f8-0636cad3de28"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Мне нравится, что вы \\nне \\nзабываете  о\\nнеобходимости \\nпомнить \\nо том, что \\nвы \\nсуществовали.\\n\\n- Я \\nне \\nзабываю  об\\nэтом,'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tr.AutoTokenizer.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
    "gpt = tr.GPT2LMHeadModel.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
    "res = gpt.generate(\n",
    "    **tokenizer(\"Мне нравится, что вы \", return_tensors=\"pt\"),\n",
    "    max_new_tokens=50,\n",
    "    top_k=3,\n",
    "    do_sample=True\n",
    ")\n",
    "tokenizer.decode(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616cda96-ac15-49fe-ad2c-d7762cf9bf8d",
   "metadata": {
    "id": "616cda96-ac15-49fe-ad2c-d7762cf9bf8d"
   },
   "source": [
    "На самом деле качество модели *очень сильно* зависит от количества параметров, и тот факт, что мы взяли модель **ruGPTsmall** сказывается на качестве текста. Но зато и процесс обучения будет существенно быстрее!\n",
    "\n",
    "Поскольку мы теперь используем другой токенизатор, то нам нужно заново токенизировать датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6b616-0bb8-4ade-89c6-846643699ac0",
   "metadata": {
    "id": "66a6b616-0bb8-4ade-89c6-846643699ac0"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"text\", data_files=\"dataset.txt\")\n",
    "ds = dataset.map(lambda x:\n",
    "                 tokenizer(x[\"text\"]), batched=True, remove_columns=[\"text\"])\n",
    "dsb = ds.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f324969-e2ed-4085-862d-906ab5bf9777",
   "metadata": {
    "id": "6f324969-e2ed-4085-862d-906ab5bf9777"
   },
   "source": [
    "Сам по себе процесс запуска обучения и указания параметров ничем не отличается от обучения трансформерной модели \"с нуля\". Возможно, при до-обучении имеет смысл указывать чуть более низкий `learning_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced1c5f-8a27-47c0-9dd4-276f5cd33f2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "executionInfo": {
     "elapsed": 952321,
     "status": "ok",
     "timestamp": 1737290664895,
     "user": {
      "displayName": "Xeem Pad",
      "userId": "14219459591791707475"
     },
     "user_tz": -180
    },
    "id": "2ced1c5f-8a27-47c0-9dd4-276f5cd33f2d",
    "outputId": "75d48d47-fecc-4380-bddb-a1c6071f6637"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-c8a8530a8982>:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = tr.Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1639' max='2232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1639/2232 43:13 < 15:39, 0.63 it/s, Epoch 5.87/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.899000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2232' max='2232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2232/2232 59:06, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.802300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2232, training_loss=2.983157715062514, metrics={'train_runtime': 3547.6508, 'train_samples_per_second': 5.031, 'train_steps_per_second': 0.629, 'total_flos': 6375933849600000.0, 'train_loss': 2.983157715062514, 'epoch': 8.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs = tr.TrainingArguments(\n",
    "    output_dir=\"gpt2-finetune\",\n",
    "    num_train_epochs=30,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=200,\n",
    "    save_steps=1500,\n",
    ")\n",
    "trainer = tr.Trainer(\n",
    "    gpt,\n",
    "    args=targs,\n",
    "    train_dataset=dsb[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=tr.default_data_collator,  # tr.DataCollatorForLanguageModeling(tokenizer=ttokenizer,mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9e6b4-eee1-4071-9aa1-a4b5a96a1c0a",
   "metadata": {
    "id": "9dc9e6b4-eee1-4071-9aa1-a4b5a96a1c0a"
   },
   "source": [
    "Смотрим на результат генерации после обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b3d10-d230-4e55-a6ae-645630603dac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "executionInfo": {
     "elapsed": 3316,
     "status": "ok",
     "timestamp": 1737291397287,
     "user": {
      "displayName": "Xeem Pad",
      "userId": "14219459591791707475"
     },
     "user_tz": -180
    },
    "id": "548b3d10-d230-4e55-a6ae-645630603dac",
    "outputId": "7223ee0f-5331-43d5-ed39-15079eb64d2d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Идите на  войну! --  закричал  он,  указывая  на  французов,которые, не обращая  внимания на него, бежали за ним,  и,  не  видя  его,  бежали  за  другими  французами,которые, не  обращая  на него  никакого внимания, бежали за ним.--  Vive  c'est une  malade!  [43] --  закричал  он,  и,  не  обращая  нанего \""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = gpt.generate(\n",
    "    **tokenizer(\"Толстой \", return_tensors=\"pt\").to(\"cuda\"),\n",
    "    max_new_tokens=100,\n",
    "    top_k=3,\n",
    "    do_sample=True\n",
    ")\n",
    "tokenizer.decode(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdac8e-d3db-459f-a864-6c693a642a2e",
   "metadata": {
    "id": "a7bdac8e-d3db-459f-a864-6c693a642a2e"
   },
   "source": [
    "Кажется, что мы получили сильно более хороший результат!\n",
    "\n",
    "## Параллелизация обучения\n",
    "\n",
    "Надеюсь, вы убедились, что на DataSphere можно обучать достаточно мощные модели, однако время, затрачиваемое на обучение, всё ещё остаётся большим. Чтобы ускорить этот процесс, обычно используют параллельное обучение на нескольких GPU одновременно.\n",
    "\n",
    "Самым распространённым вариантом параллелизма является параллелизм по данным (Data Parallel Training), в котором на каждый из обучающих GPU подаётся свой поток данных (т.е. своя часть исходного датасета). При этом на каждом обучающем шаге каждый GPU вычисляет свой градиент ошибки, которые затем усредняются и используются для синхронного обновления моделей на всех обучающих процессорах.\n",
    "\n",
    "Различают два варианта обучения на нескольких GPU:\n",
    "* **Data Parallel** - обычно используется, когда несколько GPU установлены на одном компьютере. В этом случае используется почти такой же код обучения на Python, как для однопроцессорного варианта, модель оборачивается в класс `torch.nn.DataParallel`, и минибатч распределяется по нескольким доступным на данном компьютере GPU.\n",
    "* **Distributed Data Parallel** используется в более общем случае, когда есть кластер из компьютеров с GPU.\n",
    "\n",
    "Подробнее про параллельное обучения можно почитать [в руководстве PyTorch](https://pytorch.org/docs/stable/distributed.html#distributed-basics).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c51b4-ae57-4c1a-997e-5d71eb820256",
   "metadata": {
    "id": "179c51b4-ae57-4c1a-997e-5d71eb820256"
   },
   "source": [
    "## Заключение\n",
    "\n",
    "Одна из целей данной работы заключалась в том, чтобы продемонстрировать, что обучение сложных языковых моделей с помощью современных библиотек является сравнительно простой задачей - но требующей значительных вычислительных ресурсов. Как только мы выходим за рамки вычислений, которые можно сделать за несколько часов на общедоступных инструментах типа Google Colab - у нас возникает потребность в облачных вычислительных ресурсах.\n",
    "\n",
    "Yandex DataSphere обеспечивает легкий переход от локального Jupyter Notebook или публичного облака Google Colab / Kaggle к выделенной облачной инфраструктуре в Yandex Cloud. В DataSphere вы можете:\n",
    "\n",
    "* легко настроить подключения к облачным хранилищам данных,\n",
    "* взаимодействовать с другими участниками проекта\n",
    "* использовать GitHub для контроля версий кода\n",
    "* бережливо расходовать ресурсы благодаря режиму Serverless или возможности легкого переключения между виртуальными вычислителями\n",
    "\n",
    "Для эффективной работы в DataSphere в ней необходимо немного привыкнуть, но когда этап привыкания пройдёт - вы сможете эффективно пользоваться этим инструментом и получать удовольствие от работы в нём!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "notebookId": "1481bb9f-dbbc-4e48-8133-aa84fa48d93b",
  "notebookPath": "sda-homeworks/train-trans/train-trans.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
